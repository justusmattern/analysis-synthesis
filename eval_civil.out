2022-01-03 10:36:03.809929: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-01-03 10:36:03.809983: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
There are  1  available GPUs!
Using GPU devices 0
Current single GPU: 0
Loading models...
gpt2_params: 124439808
VAE_params: 182914560
VAE_params: 182914560
Setup data...
Batch schedule [(1, 70)]
Loading civicomments dataset...
Train dataset size 201
Done.
Wrapping models and optimizers...
Selected optimization level O0:  Pure FP32 training.

Defaults for this optimization level are:
enabled                : True
opt_level              : O0
cast_model_type        : torch.float32
patch_torch_functions  : False
keep_batchnorm_fp32    : None
master_weights         : False
loss_scale             : 1.0
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O0
cast_model_type        : torch.float32
patch_torch_functions  : False
keep_batchnorm_fp32    : None
master_weights         : False
loss_scale             : 1.0
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")
Selected optimization level O0:  Pure FP32 training.

Defaults for this optimization level are:
enabled                : True
opt_level              : O0
cast_model_type        : torch.float32
patch_torch_functions  : False
keep_batchnorm_fp32    : None
master_weights         : False
loss_scale             : 1.0
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O0
cast_model_type        : torch.float32
patch_torch_functions  : False
keep_batchnorm_fp32    : None
master_weights         : False
loss_scale             : 1.0
Done.
Begin training iterations
evaluating models for epoch 2
  0%|          | 0/201 [00:00<?, ?it/s]/home/zjin/users/justus_mattern/anaconda3/envs/transformer-vae/lib/python3.7/site-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
  0%|          | 1/201 [01:20<4:27:14, 80.17s/it]  1%|          | 2/201 [02:39<4:23:22, 79.41s/it]  1%|▏         | 3/201 [04:03<4:29:37, 81.71s/it]  2%|▏         | 4/201 [05:20<4:22:10, 79.85s/it]  2%|▏         | 5/201 [06:36<4:16:36, 78.56s/it]  3%|▎         | 6/201 [07:52<4:12:05, 77.56s/it]  3%|▎         | 7/201 [09:07<4:08:17, 76.79s/it]  4%|▍         | 8/201 [10:24<4:07:05, 76.82s/it]  4%|▍         | 9/201 [11:39<4:03:35, 76.12s/it]  5%|▍         | 10/201 [12:47<3:54:17, 73.60s/it]  5%|▌         | 11/201 [13:51<3:44:16, 70.82s/it]  6%|▌         | 12/201 [15:09<3:50:01, 73.03s/it]  6%|▋         | 13/201 [16:26<3:52:34, 74.22s/it]  7%|▋         | 14/201 [17:43<3:53:54, 75.05s/it]  7%|▋         | 15/201 [19:06<3:59:49, 77.36s/it]  8%|▊         | 16/201 [20:28<4:03:00, 78.81s/it]  8%|▊         | 17/201 [21:49<4:03:51, 79.52s/it]  9%|▉         | 18/201 [23:08<4:01:49, 79.29s/it]  9%|▉         | 19/201 [24:28<4:01:34, 79.64s/it] 10%|▉         | 20/201 [25:46<3:58:14, 78.98s/it] 10%|█         | 21/201 [27:05<3:56:43, 78.91s/it] 11%|█         | 22/201 [28:22<3:54:33, 78.62s/it] 11%|█▏        | 23/201 [29:41<3:52:44, 78.45s/it] 12%|█▏        | 24/201 [31:02<3:54:30, 79.49s/it] 12%|█▏        | 25/201 [32:25<3:55:42, 80.35s/it] 13%|█▎        | 26/201 [33:47<3:56:00, 80.92s/it] 13%|█▎        | 27/201 [35:06<3:52:53, 80.31s/it] 14%|█▍        | 28/201 [36:28<3:52:55, 80.79s/it] 14%|█▍        | 29/201 [37:51<3:53:35, 81.48s/it] 15%|█▍        | 30/201 [39:10<3:50:18, 80.81s/it] 15%|█▌        | 31/201 [40:25<3:43:55, 79.03s/it]